{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ef5e87",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "Embeddings are numerical vector representations of text that capture its meaning, context, and semantics. <br> They allow a computer to \"understand\" text in a way that makes similar meanings produce similar vectors.\n",
    "\n",
    "Think of embeddings as turning words or sentences into coordinates in a high-dimensional space (e.g. 768-dimensional),<br> where closer vectors mean more similar meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c11d4",
   "metadata": {},
   "source": [
    "### Ollama embeddings\n",
    "Ollama supports embedding models, making it possible to build retrieval augmented generation (RAG) applications that combine text prompts with existing documents or other data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e3b50",
   "metadata": {},
   "source": [
    "## load and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95167724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs = TextLoader(\"embedded_techqniues.txt\").load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedb41f",
   "metadata": {},
   "source": [
    "##  embed the chunks\n",
    "What these vectors mean:  \n",
    "Each chunk is converted to a vector (e.g. 768-dimensional list of floats)<br>\n",
    "The vectors capture the semantic meaning of each chunk<br>\n",
    "Similar chunks will have vectors that are close together in vector space <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8af940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "vectors = embedding_model.embed_documents([doc.page_content for doc in chunks])\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277020c",
   "metadata": {},
   "source": [
    "### Similarity Search\n",
    "You can now search \"What is LangChain?\" and match it to semantically similar chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e28d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain continues to evolve as new models and tools emerge, making it a powerful toolkit for anyone working in the LLM application space.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "results = vector_store.similarity_search(\"How does LangChain work?\", k=3)\n",
    "print(results[0].page_content)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6cf8068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Mount Everest is the tallest mountain\n",
      "\n",
      "--- Result 2 ---\n",
      "developers to chain together various components such as prompt templates, document loaders, vector stores, and retrieval mechanisms.\n",
      "\n",
      "--- Result 3 ---\n",
      "- Customizable memory, tool usage, and agent behavior.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "results = vector_store.similarity_search(\"Mount Everest\", k=3)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\\n{r.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23378a99",
   "metadata": {},
   "source": [
    "### Other embedding models\n",
    "https://ollama.com/blog/embedding-models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
